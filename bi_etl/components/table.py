"""
Created on Sep 17, 2014

@author: woodd
"""
import codecs
import math
import os
import subprocess
import tempfile
import textwrap
import traceback
import warnings
from datetime import datetime, date, time, timedelta
from decimal import Decimal
from decimal import InvalidOperation
from typing import Iterable, Union, Callable, List

import sqlalchemy
from bi_etl import Timer
from bi_etl.components.etlcomponent import ETLComponent
from bi_etl.components.readonlytable import ReadOnlyTable, NoResultFound
from bi_etl.components.row.column_difference import ColumnDifference
from bi_etl.components.row.row import Row
from bi_etl.components.row.row_iteration_header import RowIterationHeader
from bi_etl.components.row.row_status import RowStatus
from bi_etl.conversions import nvl
from bi_etl.conversions import replace_tilda
from bi_etl.conversions import str2date
from bi_etl.conversions import str2datetime
from bi_etl.conversions import str2decimal
from bi_etl.conversions import str2float
from bi_etl.conversions import str2int
from bi_etl.conversions import str2time
from bi_etl.exceptions import ColumnMappingError
from bi_etl.statement_queue import StatementQueue
from bi_etl.statistics import Statistics
from bi_etl.utility import dict_to_str
from bi_etl.utility import getIntegerPlaces
from sqlalchemy.sql.expression import bindparam


class Table(ReadOnlyTable):
    """
    A class for accessing and updating a table.
    
    Parameters
    ----------
    task : ETLTask
        The  instance to register in (if not None)
    
    database : bi_etl.scheduler.task.Database
        The database to find the table/view in.
    
    table_name : str
        The name of the table/view.        
        
    exclude_columns
        Optional. A list of columns to exclude from the table/view. These columns will not be included in SELECT, INSERT, or UPDATE statements.    
    
    Attributes
    ----------
    
    auto_generate_key: boolean
        Should the primary key be automatically generated by the insert/upsert process?
        If True, the process will get the current maximum value and then increment it with each insert.
        
    autocommit: boolean
        Automatically commit after delete? Defaults to False.
    
    batch_size: int
        How many rows should be insert/update/deleted in a single batch.
        Default = 5000. Assigning None will use the default.
        
    delete_flag : str
        The name of the delete_flag column, if any.
        (inherited from ReadOnlyTable)
        
    delete_flag_yes : str, optional
        The value of delete_flag for deleted rows.
        (inherited from ReadOnlyTable)
        
    delete_flag_no : str, optional
        The value of delete_flag for *not* deleted rows.
        (inherited from ReadOnlyTable)
        
    default_date_format: str
        The date parsing format to use for str -> date conversions. 
        If more than one date format exists in the source, then explicit conversions will be required.
        
        Default = '%m/%d/%Y'
        
    default_date_time_format: str
        The date+time parsing format to use for str -> date time conversions. 
        If more than one date format exists in the source, then explicit conversions will be required.
        
        Default = '%m/%d/%Y %H:%M:%S'
        
    default_time_format: str
        The time parsing format to use for str ->time conversions. 
        If more than one date format exists in the source, then explicit conversions will be required.
        
        Default = '%H:%M:%S'
        
    force_ascii: boolean
        Should text values be forced into the ascii character set before passing to the database?
        Default = False
        
    last_update_date: str
        Name of the column which we should update when table updates are made.
        Default = None
            
    log_first_row : boolean
        Should we log progress on the the first row read. *Only applies if Table is used as a source.*
        (inherited from ETLComponent)

    max_rows : int, optional
        The maximum number of rows to read. *Only applies if Table is used as a source.*
        (inherited from ETLComponent)
    
    primary_key
        The name of the primary key column(s). Only impacts trace messages.  Default=None.
        If not passed in, will use the database value, if any.
        (inherited from ETLComponent)
    
    progress_frequency : int, optional
        How often (in seconds) to output progress messages.
        (inherited from ETLComponent)
    
    progress_message : str, optional
        The progress message to print. 
        Default is ``"{logical_name} row # {row_number}"``. Note ``logical_name`` and ``row_number``
        substitutions applied via :func:`format`.
        (inherited from ETLComponent)
    
    special_values_descriptive_columns
         A list of columns that get longer descriptive text in :meth:`get_missing_row`, 
         :meth:`get_invalid_row`, :meth:`get_not_applicable_row`, :meth:`get_various_row`
         (inherited from ReadOnlyTable)
        
    track_source_rows: boolean
        Should the :meth:`upsert` method keep a set container of source row keys that it has processed?  
        That set would then be used by :meth:`update_not_processed`, :meth:`logically_delete_not_processed`,
        and :meth:`delete_not_processed`.
         
    """    
    DEFAULT_BATCH_SIZE = 5000
    # Replacement for float Not a Number (NaN) values
    NAN_REPLACEMENT_VALUE = None

    def __init__(self,
                 task,
                 database,
                 table_name,
                 table_name_case_sensitive = False,
                 exclude_columns = None,
                 **kwargs
                 ):
        # Don't pass kwargs up. They should be set here at the end
        super(Table, self).__init__(task= task,
                                    database= database, 
                                    table_name= table_name,
                                    table_name_case_sensitive= table_name_case_sensitive,
                                    exclude_columns= exclude_columns,
                                    )
        self.load_via_bcp = False
        self.bcp_table_name = 'dbo.' + self.table.name
        self.bcp_update_table_name = self.table_name + '_UPD'
        self._bcp_update_table_dict = dict()
        self.track_source_rows = False
        self.auto_generate_key = False
        self.last_update_date = None
        self.default_date_format = '%m/%d/%Y'
        self.default_date_time_format = '%m/%d/%Y %H:%M:%S'
        self.default_time_format = '%H:%M:%S'         
        self.force_ascii = False
        codecs.register_error('replace_tilda', replace_tilda)
        self.autocommit = False
        self.__batch_size = self.DEFAULT_BATCH_SIZE
        self.__transaction = None

        self._logical_delete_update = None
        
        # Safe type mode is slower, but gives better error messages than the database
        # that will likely give a not-so helpful message or silently truncate a value.
        self.safe_type_mode = True

        # Init table "memory"
        self.max_keys = dict()

        # Make a self example source row
        self._example_row = self.Row()
        if self.columns is not None:
            for c in self.columns:
                self._example_row[c] = None
        
        self.source_keys_processed = set()
        
        self.insert_hint = None
        # A list of any pending rows to be inserted
        self.pending_insert_stats = None
        self.pending_insert_rows = list()
        
        # A StatementQueue of any pending delete statements, the queue has the
        # statement itself (based on the keys), and a list of pending values
        self.pending_delete_stats = None
        self.pending_delete_statements = StatementQueue()
        
        # A list of any pending rows to apply as updates
        self.pending_update_stats = None
        self.pending_update_rows = list()
        
        self.sanity_check_done = False
        self.ignore_source_not_in_target = False
        self.ignore_target_not_in_source = False
        self.raise_on_source_not_in_target = False
        self.raise_on_target_not_in_source = False
        
        # Should be the last call of every init            
        self.set_kwattrs(**kwargs)
    
    def close(self):                
        super(Table, self).close()        
        self.clear_cache()
        # Clean out update temp tables
        for update_table_object, pending_rows in self._bcp_update_table_dict:
            self.database.execute("DROP TABLE {}".format(update_table_object.table_name))

    def __iter__(self) -> Iterable(Row):
        # Note: yield_per will break if the transaction is committed while we are looping
        for row in self.where(None):
            yield row
            
    @property 
    def batch_size(self):
        return self.__batch_size 
    
    @batch_size.setter
    def batch_size(self, batch_size):
        if batch_size is not None:
            if batch_size > 0:
                self.__batch_size = batch_size
            else:
                self.__batch_size = 1

    def autogenerate_sequence(self, row, seq_column, force_override= True):
        # Make sure we have a column object
        seq_column = self.get_column(seq_column)
        # If key value is not already set, or we are supposed to force override                    
        if row.get(seq_column.name) is None or force_override:
            # Get the database max
            current_max = self.max_keys.get(seq_column)
            if current_max is None:
                current_max = self.max(seq_column)
                # In case the database call returns None
                if current_max is None:
                    current_max = 0
                    self.log.info("Initialize sequence for {} with value {:}".format(
                        seq_column,
                        current_max + 1
                        )
                    )
                # Make sure max is an integer
                current_max = int(current_max)
                # Check for negative (special values) set max to 0
                if current_max < 0:
                    current_max = 0
                    self.log.info("Initialize sequence for {} with value {:} (and not negative value)".format(
                        seq_column,
                        current_max + 1
                        )
                    )
            current_max += 1
            row[seq_column] = current_max
            # self.log.debug("{} Value = {}  =  {}".format(seq_column, row[seq_column], current_max))
            self.max_keys[seq_column] = current_max
            return current_max
    
    def autogenerate_key(self, row, force_override=True):
        if self.auto_generate_key:
            
            if len(self.primary_key) > 1:
                raise AssertionError("Can't auto generate a compound key with table {} pk={}".format(self,
                                                                                                 self.primary_key)
                                 )
            key = list(self.primary_key)[0]
            return self.autogenerate_sequence(row, seq_column= key, force_override= force_override)
            
    def sanity_check_source_mapping(self, 
                             source_definition,
                             source_name = None,
                             source_excludes = None,
                             target_excludes = None,
                             ignore_source_not_in_target = None,
                             ignore_target_not_in_source = None,
                             raise_on_source_not_in_target = None,
                             raise_on_target_not_in_source = None,
                             ):
        
        self.sanity_check_done = True
        
        if ignore_source_not_in_target is None:
            ignore_source_not_in_target = self.ignore_source_not_in_target
        if ignore_target_not_in_source is None: 
            ignore_target_not_in_source = self.ignore_target_not_in_source
        if raise_on_source_not_in_target is None:
            raise_on_source_not_in_target = self.raise_on_source_not_in_target
        if raise_on_target_not_in_source is None: 
            raise_on_target_not_in_source = self.raise_on_target_not_in_source
        
        target_set = set(self.column_names)
        target_col_list = list(self.column_names)
        if target_excludes is not None:
            for exclude in target_excludes:
                if exclude is not None:
                    if exclude in target_set:
                        target_set.remove(exclude)

        if isinstance(source_definition, ETLComponent):
            source_col_list = source_definition.column_names
            if source_name is None:
                source_name = str(source_definition) 
        elif isinstance(source_definition, Row):
            source_col_list = source_definition
        elif isinstance(source_definition, set):
            source_col_list = list(source_definition)
        elif isinstance(source_definition, list):
            source_col_list = source_definition         
        else:
            self.log.error(
                "check_column_mapping source_definition needs to be ETLComponent, Row, set, or list. Got {}".format(
                    type(source_definition)
                )
            )
            return False        
        
        if source_name is None:
            source_name = ''    
        
        source_set = set(source_col_list)
        if not ignore_source_not_in_target:
            if source_excludes is None:
                source_excludes = list()
            pos = 0
            for src_col in source_col_list:
                pos += 1
                if src_col not in source_excludes:
                    if src_col not in target_set:
                        if isinstance(source_definition, set):
                            pos = 'N/A'
                        msg = "Sanity Check: Source {src} contains column {col}({pos}) not in target {tgt}".format(
                            src=source_name,
                            col=src_col,
                            pos=pos,
                            tgt=self
                        )
                        if raise_on_source_not_in_target:
                            raise ColumnMappingError(msg)
                        else:
                            self.log.warning(msg)
        
        if self.auto_generate_key:
            if self.primary_key is not None:
                key = self.get_column_name(list(self.primary_key)[0])
            else:
                raise KeyError('Cannot generate key values without a primary key.')
        else:
            key = None
            
        if not ignore_target_not_in_source:
            for tgtCol in target_set:
                if tgtCol not in source_set:
                    if tgtCol == key:
                        pass
                    else:
                        if tgtCol not in [self.delete_flag, self.last_update_date]:
                            pos = target_col_list.index(tgtCol)
                            msg = "Sanity Check: Target {tgt} contains column {col}(col {pos}) not in source {src}"
                            msg = msg.format(
                                    tgt=self,
                                    col=tgtCol,
                                    pos=pos,
                                    src=source_name
                                    )
                            if raise_on_target_not_in_source:
                                raise ColumnMappingError(msg) 
                            else:
                                self.log.warning(msg)
    
    def sanity_check_example_row(self,
                                 example_source_row,
                                 source_excludes = None,
                                 target_excludes = None,
                                 ignore_source_not_in_target = None,
                                 ignore_target_not_in_source = None,
                                 ):
        self.sanity_check_source_mapping(example_source_row.columns,
                                         example_source_row.name,
                                         source_excludes= source_excludes,
                                         target_excludes= target_excludes,
                                         ignore_source_not_in_target= ignore_source_not_in_target,
                                         ignore_target_not_in_source= ignore_target_not_in_source,
                                         )
        # TODO: Sanity check primary key data types.
        # Lookups might fail if the types don't match (although build_row in safe_mode should fix it)

    def _trace_data_type(self, target_name, t_type, target_column_value):
        try:
            self.log.debug("{} t_type={}".format(target_name, t_type))
            self.log.debug("{} t_type.precision={}".format(target_name, t_type.precision))
            self.log.debug("{} target_column_value={}"
                           .format(target_name, target_column_value))
            self.log.debug("{} getIntegerPlaces(target_column_value)={}"
                           .format(target_name, getIntegerPlaces(target_column_value)))
            self.log.debug("{} (t_type.precision - t_type.scale)={}"
                           .format(target_name,
                                   (nvl(t_type.precision, 0) - nvl(t_type.scale, 0))))
        except AttributeError as e:
            self.log.error(traceback.format_exc())
            self.log.debug(repr(e))
    
    def build_row(self,
                  source_row: Row,
                  additional_values: dict = None,
                  source_excludes: Iterable = None,
                  target_excludes: Iterable = None,
                  stat_name: str = 'build rows',
                  parent_stats: Statistics = None,
                  ) -> Row:
        """
        Use a source row to build a row with correct data types for this table.

        Parameters
        ----------
        source_row
        additional_values
        source_excludes
        target_excludes
        stat_name
            Name of this step for the ETLTask statistics. Default = 'upsert_by_pk'
        parent_stats

        Returns
        -------
        Row
        """
        build_row_stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        build_row_stats.print_start_stop_times = False
        build_row_stats.timer.start()
        build_row_stats['calls'] += 1

        # Check if we can skip this expensive logic by cloning the row
        if (    hasattr(source_row, 'name')
            and source_row.name == self.row_name
            and source_row.column_set == self._example_row.column_set
           ):
            build_row_stats.timer.stop()            
            return source_row.subset(exclude=source_excludes)
        else: 
            #####################
            # Expensive row building is required
            #####################
            # First make sure we have an instance of Row as a source 
            if not isinstance(source_row, Row):
                # This will be slow if passed a lot of these non-Row objects
                iteration_header = RowIterationHeader(logical_name='default')
                source_row = Row(iteration_header, data=source_row)
            target_set = set(self.column_names)
            if target_excludes is not None:
                target_set -= set(target_excludes)
            new_row = source_row.subset(exclude=source_excludes, keep_only= target_set)

            # Safe type mode is slower, but gives better error messages than the 
            # database that will likely give a not-so helpful message or silently truncate a value.
            if self.safe_type_mode:             
                for target_name, target_column_value in new_row.items():
                    target_column_object = self.get_column(target_name)
                    if target_column_value is not None:
                        # Normalize the column names to the target column names (case changes)
                        # DW: Removed this for now since Row is not case sensitive
                        #if target_name != target_column_object.name:
                        #    new_row.rename_column(target_name, target_column_object.name, ignore_missing= False)
                        #    target_name = target_column_object.name
                        
                        t_type = target_column_object.type
                        type_error = False
                        err_msg = None
                    
                        try:
                            if t_type.python_type == str:
                                if isinstance(target_column_value, str):
                                    if self.force_ascii:
                                        # Passing ascii bytes to cx_Oracle is not working.
                                        # We need to pass a str value.
                                        # So we'll use encode with 'replace' to force ascii compatibility                                                                        
                                        target_column_value = \
                                            target_column_value.encode('ascii', 'replace_tilda').decode('ascii')
                                        new_row[target_name] = target_column_value
                                    # else we don't update the value
                                elif isinstance(target_column_value, bytes):
                                    target_column_value = target_column_value.decode('ascii')
                                    new_row[target_name] = target_column_value
                                else:
                                    target_column_value = str(target_column_value)
                                    new_row[target_name] = target_column_value
                                #Note: t_type.length is None for CLOB fields
                                try:
                                    if t_type.length is not None and len(target_column_value) > t_type.length:
                                        type_error = True
                                        err_msg = "length {} > {} limit".format(len(target_column_value), t_type.length)
                                except TypeError:
                                    # t_type.length is not a comparable type
                                    pass
                            elif t_type.python_type == bytes:
                                if isinstance(target_column_value, str):
                                    target_column_value = target_column_value.encode('utf-8')
                                    new_row[target_name] = target_column_value
                                elif isinstance(target_column_value, bytes):
                                    pass
                                    # We don't update the value
                                else:
                                    target_column_value = str(target_column_value).encode('utf-8')
                                    new_row[target_name] = target_column_value
                                # t_type.length is None for BLOB, LargeBinary fields.
                                # This really might not be required since all
                                # discovered types with python_type == bytes:
                                # have no length
                                if t_type.length is not None:
                                    if len(target_column_value) > t_type.length:
                                        type_error = True
                                        err_msg = "{} > {}".format(len(target_column_value), t_type.length)
                            elif t_type.python_type == int:
                                if isinstance(target_column_value, str):
                                    # Note: str2int takes 590 ns vs 220 ns for int() but handles commas and signs.                                
                                    target_column_value = str2int(target_column_value)
                                    new_row[target_name] = target_column_value
                            elif t_type.python_type == float:
                                if isinstance(target_column_value, str):
                                    # Note: str2float takes 635 ns vs 231 ns for float() but handles commas and signs.
                                    # The thought is that ETL jobs that need the performance and can guarantee no commas
                                    # can explicitly use float
                                    target_column_value = str2float(target_column_value)
                                    new_row[target_name] = target_column_value
                                elif math.isnan(target_column_value):
                                    target_column_value = self.NAN_REPLACEMENT_VALUE
                                    new_row[target_name] = target_column_value
                            elif t_type.python_type == Decimal:
                                if isinstance(target_column_value, str):
                                    # If for performance reasons you don't want this conversion...
                                    #   DON'T send in a string!
                                    # str2decimal takes 765 ns vs 312 ns for Decimal() but handles commas and signs.
                                    # The thought is that ETL jobs that need the performance and can
                                    # guarantee no commas can explicitly use float or Decimal
                                    target_column_value = str2decimal(target_column_value)
                                    new_row[target_name] = target_column_value
                                elif isinstance(target_column_value, float):
                                    if math.isnan(target_column_value):
                                        target_column_value = self.NAN_REPLACEMENT_VALUE
                                        new_row[target_name] = target_column_value
                                if t_type.precision is not None:
                                    scale = nvl(t_type.scale, 0)
                                    if getIntegerPlaces(target_column_value) > (t_type.precision - scale):
                                        type_error = True
                                        err_msg = "{digits} digits > {t_digits} = " \
                                                  "(prec {prec} - scale {scale}) limit"\
                                            .format(digits=getIntegerPlaces(target_column_value),
                                                    t_digits=(t_type.precision - scale),
                                                    prec=t_type.precision,
                                                    scale=t_type.scale,
                                                    )
                            elif t_type.python_type == date:
                                # If we already have a datetime, make it a date
                                if isinstance(target_column_value, datetime):
                                    target_column_value = date(target_column_value.year, 
                                                               target_column_value.month, 
                                                               target_column_value.day)
                                    new_row[target_name] = target_column_value
                                # If we already have a date
                                elif isinstance(target_column_value, date):
                                    pass
                                else:                            
                                    target_column_value = str2date(target_column_value,
                                                                   dt_format= self.default_date_format)
                                    new_row[target_name] = target_column_value
                            elif t_type.python_type == datetime:
                                # If we already have a date or datetime value
                                if isinstance(target_column_value, datetime) or isinstance(target_column_value, date):
                                    pass
                                elif isinstance(target_column_value, str):
                                    target_column_value = str2datetime(target_column_value,
                                                                       dt_format= self.default_date_time_format)
                                    new_row[target_name] = target_column_value
                                else:                            
                                    target_column_value = str2datetime(str(target_column_value),
                                                                       dt_format= self.default_date_time_format)
                                    new_row[target_name] = target_column_value
                            elif t_type.python_type == time:                                
                                # If we already have a datetime, make it a time
                                if isinstance(target_column_value, datetime):
                                    target_column_value = time(target_column_value.hour, 
                                                               target_column_value.minute, 
                                                               target_column_value.second,
                                                               target_column_value.microsecond,
                                                               target_column_value.tzinfo,
                                                               )
                                    new_row[target_name] = target_column_value
                                # If we already have a date or time value
                                elif isinstance(target_column_value, time):
                                    pass
                                else:                            
                                    target_column_value = str2time(target_column_value,
                                                                   dt_format= self.default_time_format)
                                    new_row[target_name] = target_column_value
                            elif t_type.python_type == timedelta:
                                # If we already have an interval value
                                if isinstance(target_column_value, timedelta):
                                    pass
                                else:                            
                                    target_column_value = timedelta(seconds=target_column_value)
                                    new_row[target_name] = target_column_value
                            elif t_type.python_type == bool:
                                if isinstance(target_column_value, bool):
                                    pass
                                elif isinstance(target_column_value, str):
                                    target_column_value = target_column_value.lower()
                                    if target_column_value in ['true', 'yes', 'y']:
                                        target_column_value = True
                                    elif target_column_value in ['false', 'no', 'n']:
                                        target_column_value = True
                                    else:
                                        type_error = True
                                        err_msg = "unexpected value (expected true/false, yes/no, y/n)"
                                    new_row[target_name] = target_column_value
                            else:
                                warnings.warn('Table.build_row has no handler for {} = {}'.format(t_type, type(t_type)))
                        except (ValueError, InvalidOperation) as e:
                            #self.log.error(traceback.format_exc())
                            self.log.error(repr(e))
                            err_msg = repr(e)
                            type_error = True
                                    
                        if type_error:
                            build_row_stats.timer.stop()
                            msg = "{table}.{column} has type {type} which cannot accept value '{val}' {err_msg}"
                            msg = msg.format(
                                table = self,
                                column = target_name,
                                type = t_type,
                                val = target_column_value,
                                err_msg = err_msg
                            )
                            raise ValueError(msg)
                # Check again for nulls in case the conversion logic made it so
                if target_column_value is None:
                        if not target_column_object.nullable:
                            msg = "{table}.{column} has is not nullable and this cannot accept value '{val}'" \
                                  " on row {row}".format(
                                    table = self,
                                    column = target_name,
                                    val = target_column_value,
                                    row = new_row.str_formatted()
                                  )
                            raise ValueError(msg)
                    
                    #End if target_column_value is not None:
                
                # End for target_name, target_column_value in new_row.items():
                
            if additional_values:
                for colName, value in additional_values.items():
                    new_row[colName] = value        
            
            build_row_stats.timer.stop()
            return new_row
    
    #===========================================================================
    # def make_bind_name(self, prefix, column_name, limit=30):
    #     full_name = prefix + column_name        
    #     if full_name in self._bind_names:
    #         result = self._bind_names[full_name]
    #     else:
    #         result = prefix + column_name[:limit-len(column_name)].upper()
    #         cnt = 2
    #         while result in self._bind_names:
    #             result = prefix + column_name[:limit-len(column_name)-len(str(cnt))].upper() + str(cnt)
    #             cnt += 1
    #     return result     
    #===========================================================================
    
    def _insert_stmt(self):
        #pylint: disable=no-value-for-parameter
        ins = self.table.insert().execution_options(autocommit=self.autocommit)
        if self.insert_hint is not None:
            ins.with_hint(self.insert_hint)
        return ins

    def _bcp(self,
             file_path,
             bcp_format_path=None,
             direction='in',
             temp_dir=None,
             start_line=1,
             ):
        if temp_dir is None:
            cleanup_temp = True
            temp_dir = tempfile.TemporaryDirectory()
        else:
            cleanup_temp = False

        bcp_errors = os.path.join(temp_dir, "bcp.errors")
        bcp_output = os.path.join(temp_dir, "bcp.output")

        cmd = [self.task.config.get_or_default('BCP', 'path', 'bcp'),
               self.bcp_table_name,
               # in / out
               direction,
               file_path,
               '-S', self.table.bind.url.host,
               '-d', self.table.bind.url.database,
               '-U', self.table.bind.url.username,
               '-P', self.table.bind.url.password,
               # Max errors
               '-m', '0',
               '-o', bcp_output,
               '-e', bcp_errors,
               '-b', '10000',
               # UTF-8
               '-C', 'UTF-8',
               # pipe delimited
               '-t', '|',
               # hints
               '-h', 'CHECK_CONSTRAINTS,TABLOCK',
               #'-h', 'CHECK_CONSTRAINTS',
               #Specify start line defaults to 1 in params
               '-F', str(start_line),
               # packet size = Max
               '-a', '65535']
        if bcp_format_path:
            cmd.extend(['-f', bcp_format_path])
        else:
            # UTF Mode
            cmd.extend(['-w'])
        self.log.debug(" ".join(cmd).replace(self.table.bind.url.password, '****'))

        try:
            rc = subprocess.check_call(cmd)
            self.log.debug('bcp rc = {}'.format(rc))
            with open(bcp_output, 'r') as bcp_output_file:
                messages = bcp_output_file.read()
            self.log.debug('BCP raw output:')
            self.log.debug(messages)
            with open(bcp_errors, 'r') as bcp_error_file:
                error_messages = bcp_error_file.read()
            self.log.debug('BCP raw errors:')
            self.log.debug(error_messages)
            messages += error_messages
            self.log.debug('BCP line by line output:')
            rows = 0
            for line in messages.split('\n'):
                self.log.debug("'{}'".format(line))
                #line.replace('\r', '')
                if line.endswith(' rows copied.'):
                    rows = int(line[:-13])
            return rows

        except IOError as e:
            raise e
        except subprocess.CalledProcessError as e:
            self.log.error("Error code " + str(e.returncode))
            self.log.error("From " + ' '.join(e.cmd))
            with open(bcp_output, 'r') as bcp_output_file:
                messages = bcp_output_file.read()
            self.log.debug('BCP raw output:')
            self.log.debug(messages)
            with open(bcp_errors, 'r') as bcp_error_file:
                error_messages = bcp_error_file.read()
            self.log.debug('BCP raw errors:')
            self.log.debug(error_messages)
            raise e
        finally:
            if cleanup_temp:
                temp_dir.cleanup()

    @staticmethod
    def _bcp_format_value(value):
        if isinstance(value, datetime):
            # noinspection PyTypeChecker,PyTypeChecker
            return ('{year:4d}-{month:02d}-{day:02d} {hour:02d}:{min:02d}:{sec:06.3f}'
                    .format(year=value.year,
                            month=value.month,
                            day=value.day,
                            hour=value.hour,
                            min=value.minute,
                            sec=value.second + value.microsecond / 1000000)
                    )
        elif value is None:
            return ''
        else:
            return str(value).replace('|', '/')

    def bcp_insert_rows(self, rows_to_insert):
        # Close connection to avoid locks
        # self.connection().close()
        if self.__transaction is not None:
            # Check if a transaction is still active.
            if self.__transaction.is_active:
                self.__transaction.commit()
        # TODO: Remove this testing code
        #bcp_dir = r"c:\temp"
        #if bcp_dir:
        with tempfile.TemporaryDirectory() as bcp_dir:
            bcp_file_path = os.path.join(bcp_dir, "bcp_file")
            bcp_format_path = os.path.join(bcp_dir, "bcp.fmt")
            with open(bcp_format_path, "w", encoding="utf-8") as bcp_fmt:
                field_list = list()
                column_list = list()
                max_col = len(self.columns)
                term = '|\\0'
                for col_num, column in enumerate(self.columns):
                    c_type = column.type
                    p_type = c_type.python_type
                    if p_type == int:
                        size = 24
                        b_type = "SQLINT"
                    elif p_type == datetime:
                        size = 48
                        b_type = "SQLDATETIME"
                    elif p_type == str:
                        size = c_type.length * 2
                        b_type = "SQLVARYCHAR"
                    else:
                        raise ValueError(
                            "bcp method not coded to support data type {type} for column {col}"
                            .format(type=p_type,
                                    col=column)
                        )
                    if col_num+1 == max_col:
                        term = "\\r\\0\\n\\0"
                    field_list.append(
                        '<FIELD ID="{col_num}" xsi:type="NCharTerm" TERMINATOR="{term}" MAX_LENGTH="{size}"/>'
                        .format(col_num=col_num+1,
                                size=size,
                                term=term
                                )
                    )
                    column_list.append(
                        '<COLUMN SOURCE="{col_num}" NAME="{name}" xsi:type="{b_type}"/>'
                        .format(col_num=col_num+1,
                                name=column.name,
                                b_type=b_type,
                                )
                    )
                bcp_fmt.write(textwrap.dedent("""\
                                <?xml version="1.0"?>
                                <BCPFORMAT xmlns="http://schemas.microsoft.com/sqlserver/2004/bulkload/format" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
                                <RECORD>
                                """))
                bcp_fmt.write('\n'.join(field_list))
                bcp_fmt.write(textwrap.dedent("""\n</RECORD>\n<ROW>"""))
                bcp_fmt.write('\n'.join(column_list))
                bcp_fmt.write(textwrap.dedent("""\n</ROW>\n</BCPFORMAT>"""))

            with open(bcp_file_path, "wt",
                      encoding='utf_16_le',
                      ) as bcp_file:
                bcp_file.write('\uFEFF')
                for row_num, row in enumerate(rows_to_insert):
                    values = [self._bcp_format_value(row.get(column_name, None))
                              for column_name in self.column_names]
                    line = '|'.join(values)
                    if row_num == 0:
                        # Write a dummy row to skip the BOM line
                        # That was causing parsing issues in BCP
                        bcp_file.write(line)
                        bcp_file.write('\r\n')
                    bcp_file.write(line)
                    bcp_file.write('\r\n')
            rows_inserted = self._bcp(bcp_file_path,
                                      bcp_format_path,
                                      start_line=2,
                                      temp_dir=bcp_dir)
            if rows_inserted != len(self.pending_insert_rows):
                self.log.error("BCP Error")
                self.log.error("Expected: {}".format(len(self.pending_insert_rows)))
                self.log.error("Actual: {}".format(rows_inserted))
                raise RuntimeError("BCP Error. See error log.")

    def _insert_pending_batch(self,
                              stat_name = 'insert',
                              parent_stats= None):
        # Need to delete pending first in case we are doing delete & insert pairs
        self._delete_pending_batch(parent_stats= parent_stats)
        # Need to update pending first in case we are doing update & insert pairs
        self._update_pending_batch(parent_stats= parent_stats)

        if len(self.pending_insert_rows) == 0:
            return
        
        if parent_stats is not None:            
            stats = parent_stats
            # Keep track of which parent last used pending inserts
            self.pending_insert_stats = stats
        else:
            stats = self.pending_insert_stats
            if stats is None:
                stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        if self.load_via_bcp:
            bcp_stats = self.get_stats_entry(stat_name + ' bcp insert', parent_stats=stats)
            bcp_stats.print_start_stop_times = False
            bcp_stats.timer.start()
            self.bcp_insert_rows(self.pending_insert_rows)
            del self.pending_insert_rows
            self.pending_insert_rows = list()
            bcp_stats.timer.stop()
        else:
            prepare_stats = self.get_stats_entry('prepare ' + stat_name, parent_stats= stats)
            prepare_stats.print_start_stop_times = False
            prepare_stats.timer.start()
            pending_insert_statements = StatementQueue()
            for new_row in self.pending_insert_rows:
                if new_row.status != RowStatus.deleted:
                    stmt_key = new_row.positioned_column_set
                    stmt = pending_insert_statements.get_statement_by_key(stmt_key)
                    if stmt is None:
                        prepare_stats['statements prepared'] += 1

                        stmt = self._insert_stmt()
                        for c in new_row:
                            col_obj = self.get_column(c)
                            stmt = stmt.values( { col_obj.name: bindparam(col_obj, type_=col_obj.type) } )
                        # this was causing InterfaceError: (cx_Oracle.InterfaceError) not a query
                        # stmt = stmt.compile()
                        pending_insert_statements.add_statement(stmt_key, stmt)
                    prepare_stats['rows prepared'] += 1
                    stmt_values = dict()
                    for c in new_row:
                        col_obj = self.get_column(c)
                        stmt_values[col_obj.name] = new_row[c]
                    pending_insert_statements.append_values_by_key(stmt_key, stmt_values)

                    # Change the row status to existing, now any updates should be via update statements
                    new_row.status = RowStatus.existing
            prepare_stats.timer.stop()
            try:
                db_stats = self.get_stats_entry(stat_name + ' database execute', parent_stats= stats)
                db_stats.print_start_stop_times = False
                db_stats.timer.start()
                rows_affected = pending_insert_statements.execute(self.connection())
                db_stats.timer.stop()
                db_stats['rows inserted'] += rows_affected
                del self.pending_insert_rows
                self.pending_insert_rows = list()
            except Exception as e:
                self.log.error(traceback.format_exc())
                self.log.error("Bulk insert failed. Applying as single inserts to find error row...")
                self.rollback()
                self.begin()
                # Retry one at a time
                for row_num, (stmt, row) in enumerate(pending_insert_statements.iter_single_statements()):
                    try:
                        # TODO: This should share the same code as insert_row's Immediate insert section
                        self.connection().execute(stmt, row)
                    except Exception as e:
                        if not isinstance(row, Row):
                            row = self.Row(row)
                        self.log.error("Error with row {row_num} stmt {stmt} stmt_values {vals}".format(
                            row_num=row_num,
                            stmt=stmt,
                            vals=row.str_formatted()
                        ))
                        raise e
                # If that didn't cause the error... re raise the original error
                self.log.error("Single inserts did not produce the error. Original error will be issued below.")
                self.rollback()
                raise e

    def insert_row(self,
                   source_row: Row,  # Must be a single row
                   additional_insert_values: dict = None,
                   build_method: str ='safe',
                   source_excludes: list = None,
                   target_excludes: list = None,
                   stat_name: str = 'insert',
                   parent_stats: Statistics = None,
                   ) -> Row:
        """
        Inserts a row into the database (batching rows as batch_size)

        Parameters
        ----------
        source_row
            The row with values to insert
        additional_insert_values
        build_method:
            'safe' matches source to tagret column by column.
            'clone' makes a clone of the source row. Prevents possible issues with outside changes to the row.
            'none' uses the row as is.
        source_excludes:
            list of source columns to exclude
        target_excludes
            list of target columns to exclude
        stat_name
        parent_stats

        Returns
        -------
        new_row
        """
        stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        stats.timer.start()
        self.begin()
        
        if not self.sanity_check_done:
            self.sanity_check_example_row(example_source_row=source_row,
                                          source_excludes=source_excludes,
                                          target_excludes=target_excludes)

        if build_method == 'none':
            new_row = source_row
        elif build_method == 'clone':
            new_row = source_row.clone()
            if additional_insert_values:
                for colName, value in additional_insert_values.items():
                    new_row[colName] = value
        else:
            new_row = self.build_row(source_row= source_row,
                                     additional_values= additional_insert_values,
                                     source_excludes= source_excludes,
                                     target_excludes= target_excludes,
                                     parent_stats = stats,
                                     )        
            
        self.autogenerate_key(new_row, force_override=False)        
        
        if self.delete_flag is not None:
            if self.delete_flag not in new_row or new_row[self.delete_flag] is None:
                new_row[self.delete_flag] = self.delete_flag_no
        
        if self.trace_data:
            self.log.debug("{} Raw row being inserted:\n{}".format(self, new_row.str_formatted()))
        
        if self.batch_size > 1:
            new_row.status = RowStatus.insert
            self.pending_insert_rows.append(new_row)
            if len(self.pending_insert_rows) >= self.batch_size:
                self._insert_pending_batch(parent_stats= stats)
        else:
            # Immediate insert
            new_row.status = RowStatus.existing
            try:
                stmt_values = dict()
                for c in new_row:
                    col_obj = self.get_column(c)
                    stmt_values[col_obj.name] = new_row[c]
                result = self.execute(self._insert_stmt(), stmt_values)
            except Exception as e:
                self.log.error("Error with source_row = {}".format(source_row.str_formatted()))
                self.log.error("Error with inserted row = {}".format(new_row.str_formatted()))
                raise e
            stats['rows inserted'] += result.rowcount
            result.close()
            
        if self.maintain_cache_during_load:            
            self.cache_row(new_row, allow_update= False)
        stats.timer.stop()
        return new_row

    def insert(self,
               source_row, # Could also be a whole list of rows
               additional_insert_values = None,
               source_renames= None,
               source_excludes= None,
               target_excludes= None,
               source_transformations= None,
               parent_stats = None,
               **kwargs
               ):
        """
        Insert a row or list of rows in the table.
        
        Parameters
        ----------
        source_row: :class:`Row` or list thereof
            Row(s) to insert
        additional_insert_values: dict
            Additional values to set on each row.
        source_renames: dict
            Renames to apply to the columns in row(s).
        source_excludes
            list of Row source columns to exclude when mapping to this Table.
        target_excludes
            list of Table columns to exclude when mapping from the source Row(s)
        source_transformations: dict or iterable container
            Container with (transform_col, transform) mappings.
            See :doc:`source_transformations`
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.                               
        """
        
        if isinstance(source_row, list):
            for row in source_row:
                self.insert_row(row, 
                                additional_insert_values= additional_insert_values,
                                source_excludes= source_excludes,
                                target_excludes= target_excludes,
                                parent_stats= parent_stats,
                                **kwargs
                                )
                return None
        else:
            return self.insert_row(source_row, 
                                   additional_insert_values= additional_insert_values, 
                                   source_excludes= source_excludes,
                                   target_excludes= target_excludes,
                                   parent_stats= parent_stats,
                                   **kwargs
                                   )

    def _delete_pending_batch(self,
                              stat_name = 'delete',
                              parent_stats = None
                              ):
        if self.pending_delete_statements.row_count > 0:
            if parent_stats is not None:
                stats = parent_stats
                # Keep track of which parent last used pending delete
                self.pending_delete_stats = stats            
            else:
                stats = self.pending_delete_stats
                if stats is None:
                    stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
            stats['rows batch deleted'] += self.pending_delete_statements.execute(self.connection())

    def _delete_stmt(self):
        # pylint: disable=no-value-for-parameter
        return self.table.delete().execution_options(autocommit=self.autocommit)
    
    def delete(self,
               key_values,
               lookup_name= None,  
               key_names = None,
               maintain_cache= None,
               stat_name = 'delete',
               parent_stats = None,
               ):
        """
        Delete rows matching key_values. 
        
        If caching is enabled, don't use key_names (use lookup_name or default to PK) or 
        an expensive scan of the cache needs to be performed.
        
        Parameters
        ----------
        key_values: dict (or list)
            Container of key values to use to identify a row(s) to delete.
        lookup_name: str
            Optional lookup name those key values are from.
        key_names
            Optional list of column names those key values are from.
        maintain_cache: boolean
            Maintain the cache when deleting. Can be expensive if key_names is used because 
            scan of the entire cache needs to be performed.
            Defaults to True
        stat_name: string
            Name of this step for the ETLTask statistics.
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.                               
            Default is to place statistics in the ETLTask level statistics.
        """
        stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        stats.timer.start()
        self.begin()
        
        if maintain_cache is None:
            maintain_cache = self.maintain_cache_during_load
        
        key_values_dict = self._generate_key_values_dict(key_names, key_values, lookup_name)
        # Don't use key_names or key_values anymore, use key_values_dict
        del key_names
        del key_values                     
        
        self.cache_clean = False
        # TODO We could try and maintain the cache here

        if self.batch_size > 1:
            # Bacth-ed deletes
            # This mode is difficult to buffer. We might have multiple delete statements used, so we have
            # to maintain a dictionary of statements by tuple(key_names) and a buffer for each
            delete_stmt_key = frozenset(key_values_dict.keys())            
            stmt = self.pending_delete_statements.get_statement_by_key(delete_stmt_key)
            if stmt is None:
                stmt = self._delete_stmt()
                
                for key_name, key_value in key_values_dict.items():
                    key  = self.get_column(key_name)
                    stmt = stmt.where(key == bindparam(key.name, type_= key.type))
                stmt = stmt.compile()
                self.pending_delete_statements.add_statement(delete_stmt_key, stmt)

            self.pending_delete_statements.append_values_by_key(delete_stmt_key, key_values_dict)
            if len(self.pending_delete_statements) >= self.batch_size:
                self._delete_pending_batch(parent_stats=stats)
        else:
            # Deletes issued as we get them
            stmt = self._delete_stmt()
            
            for key_name, key_value in key_values_dict.items():
                key  = self.get_column(key_name)
                stmt = stmt.where(key == key_value)
            
            del_result = self.execute(stmt)
            stats['rows deleted'] += del_result.rowcount
            del_result.close()

        if maintain_cache:
            if lookup_name is not None:
                full_row = self.get_by_lookup(lookup_name, key_values_dict, parent_stats= stats)
                self.uncache_row(full_row)
            else:
                self.uncache_where(key_names= key_values_dict.keys(), key_values_dict= key_values_dict)
        
        stats.timer.stop()
        
    # TODO: Add lookup parameter and change to allow set of be of that instead of keys
    def delete_not_in_set(self,
                          set_of_key_tuples: set,
                          lookup_name: str = None,
                          criteria: Union[Iterable[str], str] = None,
                          use_cache_as_source: bool = True,
                          stat_name: str = 'delete_not_in_set',
                          progress_frequency: int = None,
                          parent_stats: Statistics = None,
                          ):
        """
        WARNING: This does physical deletes !! See :meth:`logically_delete_in_set` for logical deletes.
        Deletes rows matching criteria that are not in the list_of_key_tuples pass in.
        
        Parameters
        ----------
        set_of_key_tuples
            List of tuples comprising the primary key values.
            This list represents the rows that should *not* be deleted.
        lookup_name: str
            The name of the lookup to use to find key tuples.
        criteria
            Only rows matching criteria will be checked against the ``list_of_key_tuples`` for deletion. 
            Each string value will be passed to :meth:`sqlalchemy.sql.expression.Select.where`.
            http://docs.sqlalchemy.org/en/rel_1_0/core/selectable.html?highlight=where#sqlalchemy.sql.expression.Select.where
        use_cache_as_source: bool
            Attempt to read existing rows from the cache?
        stat_name: string
            Name of this step for the ETLTask statistics. Default = 'delete_not_in_set'    
        progress_frequency: int
            How often (in seconds) to output progress messages. Default 10. None for no progress messages.
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.         
            Default is to place statistics in the ETLTask level statistics.
        """
        stats = self.get_unique_stats_entry(stat_name, parent_stats= parent_stats)
        stats['rows read'] = 0
        stats['rows deleted'] = 0
        stats.timer.start()
        deleted_rows = list()
        self.begin()

        if progress_frequency is None:
            progress_frequency = self.progress_frequency
        
        progress_timer = Timer()

        # Turn off read progress reports
        saved_progress_frequency = self.progress_frequency
        self.progress_frequency = None

        if lookup_name is None:
            lookup = self.get_nk_lookup()
        else:
            lookup = self.get_lookup(lookup_name)

        for row in self.where(criteria, use_cache_as_source=use_cache_as_source, parent_stats=stats):
            stats['rows read'] += 1
            existing_key = lookup.get_hashable_combined_key(row)

            if 0 < progress_frequency <= progress_timer.seconds_elapsed:
                progress_timer.reset()
                self.log.info("delete_not_in_set current row={} key={} deletes_done = {}"
                              .format(stats['rows read'], existing_key, stats['rows deleted']))
            if existing_key not in set_of_key_tuples:
                stats['rows deleted'] += 1
                
                deleted_rows.append(row)
                self.delete(key_values=row,
                            maintain_cache=False,
                            # We can't maintain cache here because we might be iterating over the cache
                            )
        
        for row in deleted_rows:
            self.uncache_row(row)

        stats.timer.stop()

        # Restore saved progress_frequency
        self.progress_frequency = saved_progress_frequency

    def delete_not_processed(self,
                             criteria: Union[Iterable[str], str] = None,
                             use_cache_as_source: bool = True,
                             stat_name: str ='delete_not_processed',
                             parent_stats= None
                             ):
        """
        WARNING: This does physical deletes !! See :meth:`logically_delete_not_processed` for logical deletes.
        
        Physically deletes rows matching criteria that are not in the Table memory of rows passed to :meth:`upsert`.  
        
        Parameters
        ----------
        criteria
            Only rows matching criteria will be checked against the ``list_of_key_tuples`` for deletion. 
            Each string value will be passed to :meth:`sqlalchemy.sql.expression.Select.where`.
            http://docs.sqlalchemy.org/en/rel_1_0/core/selectable.html?highlight=where#sqlalchemy.sql.expression.Select.where
        use_cache_as_source: bool
            Attempt to read existing rows from the cache?
        stat_name: string
            Name of this step for the ETLTask statistics.
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.        
        """
        assert self.track_source_rows, "delete_not_processed can't be used if we don't track source rows"
        if self.source_keys_processed is None or len(self.source_keys_processed) == 0:
            # We don't want to logically delete all the rows
            # But that's only an issue if there are target rows
            if any(True for _ in self.where(criteria=criteria)):
                raise RuntimeError("{} called before any source rows were processed.".format(stat_name))
        self.delete_not_in_set(set_of_key_tuples= self.source_keys_processed,
                               criteria= criteria,
                               stat_name= stat_name,
                               parent_stats= parent_stats)
        self.source_keys_processed = set()
        
    def logically_delete_not_in_set(self,
                                    set_of_key_tuples: set,
                                    lookup_name: str = None,
                                    criteria: Union[Iterable[str], str] = None,
                                    use_cache_as_source: bool = True,
                                    stat_name: str = 'logically_delete_not_in_set',
                                    progress_frequency: int = 10,
                                    parent_stats: Statistics = None):
        """
        Logically deletes rows matching criteria that are not in the list_of_key_tuples pass in.
        
        Parameters
        ----------
        set_of_key_tuples
            List of tuples comprising the primary key values.
            This list represents the rows that should *not* be logically deleted.
        lookup_name: str
            Name of the lookup to use
        criteria
            Only rows matching criteria will be checked against the ``list_of_key_tuples`` for deletion. 
            Each string value will be passed to :meth:`sqlalchemy.sql.expression.Select.where`.
            http://docs.sqlalchemy.org/en/rel_1_0/core/selectable.html?highlight=where#sqlalchemy.sql.expression.Select.where
        use_cache_as_source: bool
            Attempt to read existing rows from the cache?
        stat_name: string
            Name of this step for the ETLTask statistics. Default = 'delete_not_in_set'    
        progress_frequency: int
            How often (in seconds) to output progress messages. Default = 10.
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.         
            Default is to place statistics in the ETLTask level statistics.
        """
        if self._logical_delete_update is None:
            self._logical_delete_update = Row(RowIterationHeader(logical_name='logical_delete'))
            self._logical_delete_update[self.delete_flag] = self.delete_flag_yes

        if criteria is None:
            # Default to not processing rows that are already deleted
            criteria = {self.delete_flag: self.delete_flag_no}

        self.update_not_in_set(updates_to_make=self._logical_delete_update,
                               set_of_key_tuples=set_of_key_tuples,
                               lookup_name=lookup_name,
                               criteria=criteria,
                               progress_frequency=progress_frequency,
                               stat_name=stat_name,
                               parent_stats=parent_stats)
        
    def logically_delete_not_processed(self,
                                       criteria: Union[Iterable[str], str] = None,
                                       use_cache_as_source: bool = True,
                                       parent_stats= None
                                       ):
        """
        Logically deletes rows matching criteria that are not in the Table memory of rows passed to :meth:`upsert`.  
        
        Parameters
        ----------
        criteria
            Only rows matching criteria will be checked against the ``list_of_key_tuples`` for logical deletion. 
            Each string value will be passed to :meth:`sqlalchemy.sql.expression.Select.where`.
            http://docs.sqlalchemy.org/en/rel_1_0/core/selectable.html?highlight=where#sqlalchemy.sql.expression.Select.where
        use_cache_as_source: bool
            Attempt to read existing rows from the cache?
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.
        """
        assert self.track_source_rows, """
            logically_delete_not_processed can't be used if we don't track source rows
            """
        assert self.source_keys_processed, """
            logically_delete_not_processed called before any source rows were processed.
            """
        self.logically_delete_not_in_set(set_of_key_tuples=self.source_keys_processed,
                                         criteria=criteria,
                                         stat_name='logically_delete_not_processed',
                                         parent_stats=parent_stats)
        self.source_keys_processed = set()
        
    def logically_delete_not_in_source(self,
                                       source: ReadOnlyTable,
                                       source_criteria: Union[Iterable[str], str] = None,
                                       target_criteria: Union[Iterable[str], str] = None,
                                       use_cache_as_source: bool = True,
                                       parent_stats: Statistics = None):
        """
        Logically deletes rows matching criteria that are not in the source component passed to this method.
        The primary use case for this method is when the upsert method is only passed new/changed records and so cannot 
        build a complete set of source keys in source_keys_processed.
        
        Parameters
        ----------
        source:
            The source to read to get the source keys.
        source_criteria
            Only source rows matching criteria will added to the list of keys not to logically delete. 
            Each string value will be passed to :meth:`sqlalchemy.sql.expression.Select.where`.
            http://docs.sqlalchemy.org/en/rel_1_0/core/selectable.html?highlight=where#sqlalchemy.sql.expression.Select.where
        target_criteria
            Only target rows matching criteria will be checked against the source keys for logical deletion. 
            Each string value will be passed to :meth:`sqlalchemy.sql.expression.Select.where`.
            http://docs.sqlalchemy.org/en/rel_1_0/core/selectable.html?highlight=where#sqlalchemy.sql.expression.Select.where
        use_cache_as_source: bool
            Attempt to read existing rows from the cache?
        parent_stats:
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.
        
        """
        self.log.info("Processing deletes")
        self.log.info("...getting source keys")                                
        set_of_source_keys = set()
        for row in source.where(column_list=source.primary_key, criteria= source_criteria, parent_stats= parent_stats):
            set_of_source_keys.add(source.get_primary_key_value_tuple(row))
        
        self.log.info("...logically_delete_not_in_set of source keys")
        self.logically_delete_not_in_set(set_of_source_keys,
                                         criteria=target_criteria,
                                         use_cache_as_source=use_cache_as_source,
                                         parent_stats=parent_stats)

    def _update_via_bcp(self, update_rows):
        for row in update_rows:
            if row.status not in [RowStatus.deleted, RowStatus.insert]:
                update_stmt_key = row.column_set
                if update_stmt_key in self._bcp_update_table_dict:
                    update_table_object, pending_rows = self._bcp_update_table_dict[update_stmt_key]
                else:
                    created = False
                    id = 0
                    table_name = None
                    while not created:
                        table_name = "tmp_" + str(datetime.now().toordinal()) + str(id)
                        try:
                            row_columns = [self.get_column(col_name).copy() for col_name in row.columns]
                            sa_table = sqlalchemy.schema.Table(table_name,
                                                               self.database,
                                                               *row_columns)
                            sa_table.create()
                            created = True
                        except (sqlalchemy.exc.OperationalError, sqlalchemy.exc.InvalidRequestError):
                            id += 1

                    update_table_object = Table(task = self.task,
                                                table_name = table_name,
                                                database = self.database,
                                                )
                    update_table_object.close()
                    if self.__transaction is not None:
                        # Check if a transaction is still active.
                        if self.__transaction.is_active:
                            self.__transaction.commit()
                            self.__transaction.close()
                    if self.is_connected:
                        self.connection().close()
                    pending_rows = list()
                    self._bcp_update_table_dict[update_stmt_key] = update_table_object, pending_rows
                pending_rows.append(row)

        for update_table_object, pending_rows in self._bcp_update_table_dict.values():
            update_table_object.truncate()
            update_table_object.bcp_insert_rows(pending_rows)
            database_type = type(self.connection().dialect).name
            if database_type == 'oracle':
                raise NotImplementedError("Oracle MERGE not yet implemented")
            elif database_type in ['mssql']:
                sets_list = ["{column} = {updates_tbl}.{column}".format(column=column,
                                                                        updates_tbl=update_table_object.table_name,
                                                                        )
                             for column in update_table_object.column_names
                            ]
                sets = ','.join(sets_list)

                key_join_list = ["{target_tbl}.{column} = {updates_tbl}.{column}"
                                     .format(column=column,
                                             target_tbl=self.table_name,
                                             updates_tbl=update_table_object.table_name,)
                                 for column in self.primary_key]
                key_joins = ','.join(key_join_list)

                sql = """\
                    UPDATE {target_tbl}
                    SET {sets}
                    FROM {target_tbl}
                         INNER JOIN
                         {updates_tbl}
                           ON {key_joins}
                    """.format(target_tbl=self.table_name,
                               sets=sets,
                               updates_tbl=update_table_object.table_name,
                               key_joins= key_joins
                               )
                self.database.execute(sql)
            else:
                raise NotImplementedError("UPDATE FROM/MERGE not yet implemented for {}".format(database_type))

    def _update_pending_batch(self,
                              stat_name: str = 'update',
                              parent_stats: Statistics = None):
        """

        Parameters
        ----------
        stat_name: str
            Name of this step for the ETLTask statistics. Default = 'update'
        parent_stats: Statistics

        Returns
        -------

        """
        if len(self.pending_update_rows) == 0:
            return
        assert self.primary_key, "add_pending_update called for table with no primary key"
        if parent_stats is not None:
            stats = parent_stats
            # Keep track of which parent stats last called us
            self.pending_update_stats = stats
        else:
            stats = self.pending_update_stats
            if stats is None:
                stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        stats.timer.start()
        self.begin()

        if self.load_via_bcp:
            self._update_via_bcp(self.pending_update_rows)
        else:
            pending_update_statements = StatementQueue()

            for row_dict in self.pending_update_rows:
                if row_dict.status not in [RowStatus.deleted, RowStatus.insert]:
                    stats['apply updates sent to db'] += 1
                    update_stmt_key = row_dict.column_set
                    update_stmt = pending_update_statements.get_statement_by_key(update_stmt_key)
                    if update_stmt is None:
                        update_stmt = self._update_stmt()
                        for key_column in self.primary_key:
                            key = self.get_column(key_column)
                            #bind_name = self.make_bind_name('k',key.name)
                            bind_name = key.name
                            # Note SQLAlchemy takes care of converting to positional “qmark” bind parameters as needed
                            update_stmt = update_stmt.where(key == bindparam(bind_name, type_= key.type))
                        for c in row_dict.columns:
                            # Since we know we aren't updating the key, don't send the keys in the values clause
                            if c not in self.primary_key:
                                #bind_name = self.make_bind_name('c',c)
                                col_obj = self.get_column(c)
                                bind_name = col_obj.name
                                update_stmt = update_stmt.values( { c: bindparam(bind_name, type_= col_obj.type) } )
                        update_stmt = update_stmt.compile()
                        pending_update_statements.add_statement(update_stmt_key, update_stmt)

                    stmt_values = dict()
                    for key_column in self.primary_key:
                        key = self.get_column(key_column)
                        #bind_name = self.make_bind_name('k',key.name)
                        bind_name = key.name
                        stmt_values[bind_name] = row_dict[key_column]

                    for c in row_dict.columns:
                        # Since we know we aren't updating the key, don't send the keys in the values clause
                        if c not in self.primary_key:
                            #bind_name = self.make_bind_name('c',c)
                            #bind_name = c
                            col_obj = self.get_column(c)
                            bind_name = col_obj.name
                            stmt_values[bind_name] = row_dict[c]

                    pending_update_statements.append_values_by_key(update_stmt_key, stmt_values)
                    row_dict.status = RowStatus.existing
            try:
                rows_applied = pending_update_statements.execute(self.connection())
                stats['apply updates db applied'] += rows_applied
                #len(self.pending_update_rows)
            except Exception as e:
                # Retry one at a time
                self.log.error("Bulk update failed. Applying as single updates to find error row...")
                for (stmt, row_dict) in pending_update_statements.iter_single_statements():
                    try:
                        self.connection().execute(stmt, row_dict)
                    except Exception as e:
                        self.log.error("Error with row {}".format(dict_to_str(row_dict)))
                        raise e
                # If that didn't cause the error... re-raise the original error
                self.log.error("Single updates did not produce the error. Original error will be issued below.")
                self.rollback()
                raise e
        del self.pending_update_rows
        self.pending_update_rows = list()
        stats.timer.stop()

    def _add_pending_update(self, row, parent_stats= None):
        assert self.primary_key, "add_pending_update called for table with no primary key"
        assert row.status != RowStatus.insert, "add_pending_update called with row that's pending insert"
        assert row.status != RowStatus.deleted, "add_pending_update called with row that's pending delete"
        self.pending_update_rows.append(row)        
        if len(self.pending_update_rows) >= self.batch_size:
            self._update_pending_batch(parent_stats= parent_stats)
    
    def apply_updates(self,
                      row,
                      changes_list= None,
                      additional_update_values= None,
                      source_effective_date=None,
                      stat_name='update',
                      parent_stats= None):
        """
        This method should only be called with a row that has already been transformed into the correct datatypes
        and column names.
        
        The update values can either already be in row or be in the changes_list.
        """
        assert self.primary_key, "apply_updates called for table with no primary key"
        assert row.status != RowStatus.deleted, "apply_updates called for deleted row {}".format(row)
            
        stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        stats.timer.start()
        
        # Set the last update date
        if self.last_update_date is not None:
            row[self.get_column_name(self.last_update_date)] = datetime.now()
            
        if changes_list is not None:
            for d_chg in changes_list:
                if isinstance(d_chg, ColumnDifference):
                    row[d_chg.column_name] = d_chg.new_value
                else:
                    row[d_chg] = changes_list[d_chg]
        if additional_update_values is not None:
            for col_name, value in additional_update_values.items():
                row[col_name] = value
        if self.maintain_cache_during_load:
            self.cache_row(row, allow_update= True)
        
        # Check that the row isn't pending an insert
        if row.status != RowStatus.insert:
            row.status = RowStatus.update_whole
            self._add_pending_update(row, parent_stats= stats)
            if stats is not None:
                stats['update called'] += 1
        else:
            if stats is not None:
                stats['updated pending insert'] += 1
        stats.timer.stop()         
    
    def _update_stmt(self):
        #pylint: disable=no-value-for-parameter
        return self.table.update().execution_options(autocommit=self.autocommit)
    
    def update_where_pk(self,
                        updates_to_make: Union[Row, dict],
                        key_values: Union[Row, dict, list] = None,
                        source_excludes: Iterable = None,
                        target_excludes: Iterable = None,
                        stat_name: str = 'update_where_pk',
                        parent_stats: Statistics = None,
               ):
        """
        Updates the table using the primary key for the where clause.
        
        Parameters
        ----------
        updates_to_make:
            Updates to make to the rows matching the criteria
            Can also be used to pass the key_values, so you can pass a single 
            :class:`~bi_etl.components.row.row_case_insensitive.Row` or ``dict``
            to the call and have it automatically get the filter values and updates from it.
        key_values:
            Optional. dict or list of key to apply as criteria
        source_excludes:
            Optional. list of :class:`~bi_etl.components.row.row_case_insensitive.Row`
            source columns to exclude when mapping to this Table.
        target_excludes:
            Optional. list of Table columns to exclude when mapping from the source
            :class:`~bi_etl.components.row.row_case_insensitive.Row` (s)
        stat_name:
            Name of this step for the ETLTask statistics. Default = 'upsert_by_pk'
        parent_stats:
            Optional. Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.
        """        
        stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        
        if not self.sanity_check_done:
            self.sanity_check_example_row(updates_to_make,
                                          source_excludes,
                                          target_excludes,
                                          ignore_target_not_in_source = True)
            
        source_mapped_as_target_row = self.build_row(source_row= updates_to_make,
                                                     source_excludes = source_excludes,
                                                     target_excludes = target_excludes,
                                                     parent_stats = stats,
                                                     )

        key_names = self.primary_key        
        key_values_dict = self._generate_key_values_dict(key_names, key_values, other_values_dict=updates_to_make)
        # Don't use key_names or key_values anymore, use key_values_dict
        del key_names
        del key_values 
        
        source_mapped_as_target_row.update(key_values_dict)        
        self.apply_updates(source_mapped_as_target_row, parent_stats= stats)
    
    def update(self,
               updates_to_make: Union[Row, dict],
               key_names: Iterable = None,
               key_values: Iterable = None,
               lookup_name: str = None,
               update_all_rows: bool = False,
               source_excludes: Iterable = None,
               target_excludes: Iterable = None,
               stat_name: str = 'direct update',
               parent_stats: Statistics = None,
               ):
        """
        Directly performs a database update. Invalidates caching.  
        If you have a full target row, use apply_updates.
        
        Parameters
        ----------
        updates_to_make:
            Updates to make to the rows matching the criteria
            Can also be used to pass the key_values, so you can pass a single 
            :class:`~bi_etl.components.row.row_case_insensitive.Row` or ``dict`` to the call and have it automatically get the
            filter values and updates from it. 
        key_names:
            Optional. List of columns to apply criteria too (see ``key_values``).
            Defaults to Primary Key columns.
        key_values:
            Optional. List of values to apply as criteria (see ``key_names``).
            If not provided, and ``update_all_rows`` is False, look in ``updates_to_make`` for values.
        lookup_name: str
            Name of the lookup to use
        update_all_rows:
            Optional. Defaults to False. If set to True, ``key_names`` and ``key_values`` are not required.
        source_excludes
            Optional. list of :class:`~bi_etl.components.row.row_case_insensitive.Row` source columns to exclude when mapping to this Table.
        target_excludes
            Optional. list of Table columns to exclude when mapping from the source :class:`~bi_etl.components.row.row_case_insensitive.Row` (s)
        stat_name: str
            Name of this step for the ETLTask statistics. Default = 'direct update'
        parent_stats: bi_etl.statistics.Statistics
            Optional. Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.
        """        
        stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        stats.timer.start()
        
        # Check if we can pass this of to update_where_pk
        if key_names is None and not update_all_rows:
            self.update_where_pk(updates_to_make= updates_to_make, 
                                 key_values= key_values, 
                                 source_excludes= source_excludes,
                                 target_excludes= target_excludes, 
                                 parent_stats= stats)
            return
        
        if not self.sanity_check_done:
            self.sanity_check_example_row(updates_to_make,
                                          source_excludes,
                                          target_excludes,
                                          ignore_target_not_in_source = True)
            
        source_mapped_as_target_row = self.build_row(source_row= updates_to_make,
                                                     source_excludes = source_excludes,
                                                     target_excludes = target_excludes,
                                                     parent_stats = stats,
                                                     )

        stmt = self._update_stmt()
        if not update_all_rows:
            key_values_dict = self._generate_key_values_dict(key_names, 
                                                             key_values,
                                                             lookup_name= lookup_name, 
                                                             other_values_dict= updates_to_make)
            # Don't use key_names or key_values anymore, use key_values_dict
            del key_names
            del key_values 

            for key_name, key_value in key_values_dict.items():
                key = self.get_column(key_name)
                stmt = stmt.where(key == key_value)
            
            # We could optionally scan the entire cache and apply updates there
            # instead for now, we'll uncache the row and set the lookups to fall back to the database
            
            # TODO: If we have a lookup based on an updated value, the original value would still be 
            # in the lookup. We don't know the original without doing a lookup.
            
            self.cache_clean = False
            if self.maintain_cache_during_load:
                self.uncache_row(key_values_dict)
        else:
            assert not key_values, "update_all_rows set and yet we got key_values"
        # End not update all rows    
        
        # Add set statements to the update
        for c in source_mapped_as_target_row:                
            v = source_mapped_as_target_row[c]
            stmt = stmt.values({c: v})
            
        stats['direct_updates_sent_to_db'] += 1 
        result = self.execute(stmt)
        stats['direct_updates_applied_to_db'] += result.rowcount
        result.close()

        stats.timer.stop()

    def update_not_in_set(self,
                          updates_to_make,
                          set_of_key_tuples: set,
                          lookup_name: str = None,
                          criteria: Union[Iterable[str], str] = None,
                          progress_frequency: int = None,
                          stat_name: str = 'update_not_in_set',
                          parent_stats: Statistics = None,
                          **kwargs):
        """
        Applies update to rows matching criteria that are not in the list_of_key_tuples pass in.
        
        Parameters
        ----------
        updates_to_make: :class:`~bi_etl.components.row.row_case_insensitive.Row`
            :class:`~bi_etl.components.row.row_case_insensitive.Row` or dict of updates to make
        set_of_key_tuples
            List of tuples comprising the primary key values. This list represents the rows that should *not* be updated.
        lookup_name: str
            The name of the lookup to use to find key tuples.
        criteria : string or list of strings 
            Only rows matching criteria will be checked against the ``list_of_key_tuples`` for deletion. 
            Each string value will be passed to :meth:`sqlalchemy.sql.expression.Select.where`.
            http://docs.sqlalchemy.org/en/rel_1_0/core/selectable.html?highlight=where#sqlalchemy.sql.expression.Select.where
        stat_name: string
            Name of this step for the ETLTask statistics. Default = 'delete_not_in_set'    
        progress_frequency: int
            How often (in seconds) to output progress messages. Default 10. None for no progress messages.
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.         
            Default is to place statistics in the ETLTask level statistics.
        """
        stats = self.get_unique_stats_entry(stat_name, parent_stats= parent_stats)
        stats['rows read'] = 0
        stats['updates count'] = 0
        stats.timer.start()
        self.begin()
        
        if progress_frequency is None:
            progress_frequency = self.progress_frequency

        progress_timer = Timer()
        
        # Turn off read progress reports
        saved_progress_frequency = self.progress_frequency
        self.progress_frequency = None
        
        if lookup_name is None:
            lookup = self.get_nk_lookup()
        else:                
            lookup = self.get_lookup(lookup_name)
        
        # Note, here we select only lookup columns from self
        for row in self.where(column_list= lookup.lookup_keys, criteria=criteria, parent_stats=stats):
            if row.status == RowStatus.unknown:
                pass
            stats['rows read'] += 1
            existing_key = lookup.get_hashable_combined_key(row)
            
            if 0 < progress_frequency <= progress_timer.seconds_elapsed:
                progress_timer.reset()
                self.log.info("update_not_in_set current current row#={} row key={} updates done so far = {}".format(
                    stats['rows read'], existing_key, stats['updates count']))
                
            if existing_key not in set_of_key_tuples:
                stats['updates count'] += 1

                # First we need the entire existing row
                target_row = self.get_by_lookup(lookup_name, row)

                # Then we can apply the updates to it
                self.apply_updates(row= target_row, additional_update_values= updates_to_make, parent_stats= stats)
        stats.timer.stop()

        # Restore saved progress_frequency
        self.progress_frequency = saved_progress_frequency 

    def update_not_processed(self,
                             update_row,
                             lookup_name = None,
                             criteria = None,
                             stat_name='update_not_processed',
                             parent_stats= None):
        """
        Applies update to rows matching criteria that are not in the Table memory of rows passed to :meth:`upsert`.  
        
        Parameters
        ----------
        update_row: :class:`~bi_etl.components.row.row_case_insensitive.Row`
            :class:`~bi_etl.components.row.row_case_insensitive.Row` or dict of updates to make
        criteria
            Only rows matching criteria will be checked against the ``list_of_key_tuples`` for update. 
            Each string value will be passed to :meth:`sqlalchemy.sql.expression.Select.where`.
            http://docs.sqlalchemy.org/en/rel_1_0/core/selectable.html?highlight=where#sqlalchemy.sql.expression.Select.where
        lookup_name: str
            Optional lookup name those key values are from.
        stat_name: str
            Name of this step for the ETLTask statistics.
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.
        """
        assert self.track_source_rows, "update_not_processed can't be used if we don't track source rows"
        if self.source_keys_processed is None or len(self.source_keys_processed) == 0:
            # We don't want to logically delete all the rows
            # But that's only an issue if there are target rows
            if any(True for _ in self.where(criteria=criteria)):
                raise RuntimeError("{} called before any source rows were processed.".format(stat_name))
        self.update_not_in_set(updates_to_make=update_row,
                               set_of_key_tuples=self.source_keys_processed,
                               lookup_name=lookup_name,
                               criteria=criteria,
                               stat_name=stat_name,
                               parent_stats=parent_stats)
        self.source_keys_processed = set()

    def upsert(self,
               source_row: Union[Row, List[Row]],
               lookup_name: str = None,
               skip_update_check_on: list = None,
               do_not_update: list = None,
               additional_update_values: dict = None,
               additional_insert_values: dict = None,
               update_callback: Callable[[list, Row], None] = None,
               insert_callback: Callable[[list, Row], None] = None,
               source_excludes: list = None,
               target_excludes: list = None,
               stat_name: str = 'upsert',
               parent_stats: Statistics = None,
               **kwargs
               ):
        """
        Update (if changed) or Insert a row in the table.
        This command will look for an existing row in the target table 
        (using the primary key lookup if no alternate lookup name is provided). If no existing row is found, an insert
        will be generated. If an existing row is found, it will be compared to the row passed in. If changes are found,
        an update will be generated.  
        
        Returns the row found/inserted, with the auto-generated key (if that feature is enabled)
        
        Parameters
        ----------
        source_row
            :class:`~bi_etl.components.row.row_case_insensitive.Row` to upsert
        lookup_name
            The name of the lookup (see :meth:`define_lookup`) to use when searching for an existing row.
        skip_update_check_on
            List of column names to not compare old vs new for updates.
        do_not_update
             List of columns to never update.
        additional_update_values
            Additional updates to apply when updating
        additional_insert_values
            Additional values to set on each row when inserting.
        update_callback: function
            Function to pass updated rows to. Function should not modify row.
        insert_callback: function
            Function to pass inserted rows to. Function should not modify row.
        source_excludes
            list of Row source columns to exclude when mapping to this Table.
        target_excludes
            list of Table columns to exclude when mapping from the source Row(s)
        stat_name
            Name of this step for the ETLTask statistics. Default = 'upsert'
        parent_stats
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.
        """
        stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        stats.ensure_exists('upsert source row count')

        stats.timer.start()
        self.begin()
        
        if not self.sanity_check_done:
            self.sanity_check_example_row(example_source_row= source_row, 
                                          source_excludes= source_excludes,
                                          target_excludes= target_excludes,
                                          )

        stats['upsert source row count'] += 1

        # Check for existing row
        target_row = None
        source_mapped_as_target_row = self.build_row(source_row= source_row,
                                                     source_excludes = source_excludes,
                                                     target_excludes = target_excludes,                                                     
                                                     parent_stats= stats,
                                                     )
        
        if self.delete_flag is not None:
            if self.delete_flag not in source_mapped_as_target_row or source_mapped_as_target_row[self.delete_flag] is None:
                source_mapped_as_target_row[self.delete_flag] = self.delete_flag_no
        
        try:
            # We'll default to using the primary key provided
            if lookup_name is None:
                lookup_name = Table.PK_LOOKUP
                if not self.primary_key:
                    raise AssertionError("upsert needs a lookup_key or a table with a primary key!")

            lookup_object = self.get_lookup(lookup_name)
            if not lookup_object.cache_enabled and self.maintain_cache_during_load and self.batch_size > 1:
                raise AssertionError("Caching needs to be turned on if batch mode is on!")

            existing_row = self.get_by_lookup(lookup_name, source_mapped_as_target_row, parent_stats= stats)
            if self.trace_data:
                lookup_keys = self.get_lookup(lookup_name).get_list_of_lookup_column_values(source_mapped_as_target_row)
            else:
                lookup_keys = None

            if do_not_update is None:
                do_not_update = []
            if skip_update_check_on is None:
                skip_update_check_on = []
            changes_list = existing_row.compare_to(source_mapped_as_target_row,
                                                   exclude= do_not_update + skip_update_check_on )
            delayed_changes = existing_row.compare_to(source_mapped_as_target_row,
                                                      compare_only= skip_update_check_on )
            if self.trace_data:
                for chg in changes_list:
                    self.log.debug("{key} {name} changed from {old} to {new}".format(
                        key=lookup_keys,
                        name=chg.column_name,
                        old=chg.old_value,
                        new=chg.new_value
                        )
                    )
                for chg in delayed_changes:
                    self.log.debug("{key} NO UPDATE TRIGGERED BY {name} changed from {old} to {new}"
                        .format(
                            key=lookup_keys,
                            name=chg.column_name,
                            old=chg.old_value,
                            new=chg.new_value
                        )
                    )
            if len(changes_list) > 0:
                self.apply_updates(row=existing_row, 
                                   changes_list=changes_list + delayed_changes, 
                                   additional_update_values=additional_update_values, 
                                   parent_stats= stats
                                   )     
            if update_callback:
                update_callback(target_row)
            target_row = existing_row
        except NoResultFound:
            new_row = source_mapped_as_target_row
            if additional_insert_values:
                for colName, value in additional_insert_values.items():
                    new_row[colName] = value
            self.autogenerate_key(new_row, force_override=True)            
            self.insert_row(new_row, build_method='none', parent_stats= stats)
            stats['insert new called'] += 1
            if insert_callback:
                insert_callback(new_row)
            target_row = new_row
        
        if self.track_source_rows:
            # Keep track of source records so we can check if target rows don't exist in source
            # Note: We use the target_row here since it has already been translated to match the target table
            # It's also important that it have the existing surrogate key (if any)
            self.source_keys_processed.add(self.get_natural_key_tuple(target_row))
            
        stats.timer.stop()
        return target_row
        
    def upsert_special_values_rows(self,
                                   stat_name = 'upsert_special_values_rows',
                                   parent_stats= None):
        """
        Send all special values rows to upsert to ensure they exist and are current.
        Rows come from :meth:`get_missing_row`, :meth:`get_invalid_row`, :meth:`get_not_applicable_row`, :meth:`get_various_row`
        
        Parameters
        ----------
        stat_name: str
            Name of this step for the ETLTask statistics. Default = 'upsert_special_values_rows'
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.       
        """
        self.log.info("Checking special values rows for {}".format(self))
        stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        stats['calls'] += 1
        stats.timer.start()
        save_auto_gen = self.auto_generate_key
        self.auto_generate_key = False
        self.upsert(self.get_missing_row(), parent_stats= stats)
        self.upsert(self.get_invalid_row(), parent_stats= stats)
        self.upsert(self.get_not_applicable_row(), parent_stats= stats)
        self.upsert(self.get_various_row(), parent_stats= stats)
        self.commit(parent_stats= stats)
        self.auto_generate_key = save_auto_gen
        stats.timer.stop()
        
    def truncate(self,
                 timeout=60,
                 stat_name= 'truncate',
                 parent_stats= None):
        """
        Truncate the table if possible, else delete all.
        
        Parameters
        ----------
        timeout: int
            How long in seconds to wait for the truncate. Oracle only.
        stat_name: str
            Name of this step for the ETLTask statistics. Default = 'truncate'
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.
        """
        stats = self.get_stats_entry(stat_name, parent_stats= parent_stats)
        stats['calls'] += 1
        stats.timer.start()
        database_type = type(self.connection().dialect).name
        if database_type == 'oracle':
            self.execute('alter session set ddl_lock_timeout={}'.format(timeout))        
            self.execute("TRUNCATE TABLE {}".format(self.table))
        elif database_type in ['mssql', 'mysql', 'postgresql', 'sybase']:
            self.execute("TRUNCATE TABLE {}".format(self.table))
        else:
            self.execute(self._delete_stmt())
        stats.timer.stop()

    def begin(self):
        if self.__transaction is None or not self.__transaction.is_active:
            self.__transaction = self.connection().begin()

    def commit(self,
               stat_name= 'commit',
               parent_stats= None,
               print_to_log= True):
        """
        Flush any buffered deletes, updates, or inserts
        
        Parameters
        ----------
        stat_name: str
            Name of this step for the ETLTask statistics. Default = 'commit'
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.
        print_to_log: bool
            Should this add a debug log entry for each commit. Defaults to true.
        """        
        # insert_pending_batch calls other *_pending methods
        self._insert_pending_batch()

        if print_to_log:
            self.log.debug("Committing {}".format(self.table_name))
        stats = self.get_unique_stats_entry(stat_name, parent_stats= parent_stats)
        # Start & stop appropriate timers for each
        stats['commit count'] += 1
        stats.timer.start()
        if self.__transaction is not None:
            # Check if a transaction is still active.
            if self.__transaction.is_active:
                self.__transaction.commit()
                self.begin()
        stats.timer.stop()
    
    def rollback(self,
                 stat_name= 'rollback',
                 parent_stats= None):
        """
        Rollback any uncommitted deletes, updates, or inserts.
        
        Parameters
        ----------
        stat_name: str
            Name of this step for the ETLTask statistics. Default = 'rollback'
        parent_stats: bi_etl.statistics.Statistics
            Optional Statistics object to nest this steps statistics in.
            Default is to place statistics in the ETLTask level statistics.       
        """    
        self.log.debug("Rolling back transaction")
        stats = self.get_unique_stats_entry(stat_name, parent_stats= parent_stats)
        stats['calls'] += 1
        stats.timer.start()
        if self.__transaction is not None:
            # Check if a transaction is still active.
            if self.__transaction.is_active:
                self.__transaction.rollback()
                self.begin()
        stats.timer.stop()
